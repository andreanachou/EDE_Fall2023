---
title: "Assignment 10: Data Scraping"
author: "Andreana Chou"
output: pdf_document
---

## OVERVIEW

This exercise accompanies the lessons in Environmental Data Analytics on data scraping. 

## Directions
1. Rename this file `<FirstLast>_A10_DataScraping.Rmd` (replacing `<FirstLast>` with your first and last name).
2. Change "Student Name" on line 3 (above) with your name.
3. Work through the steps, **creating code and output** that fulfill each instruction.
4. Be sure your code is tidy; use line breaks to ensure your code fits in the knitted output.
5. Be sure to **answer the questions** in this assignment document.
6. When you have completed the assignment, **Knit** the text and code into a single PDF file.


## Set up 
1. Set up your session:

* Load the packages `tidyverse`, `rvest`, and any others you end up using.
* Check your working directory

```{r 1. Project Setup, message = FALSE}
#1 

library(tidyverse)
library(lubridate)
library(rvest)
library(here)

here()

```

2. We will be scraping data from the NC DEQs Local Water Supply Planning website, specifically the Durham's 2022 Municipal Local Water Supply Plan (LWSP): 
 * Navigate to https://www.ncwater.org/WUDC/app/LWSP/search.php
 * Scroll down and select the LWSP link next to Durham Municipality. 
 * Note the web address: <https://www.ncwater.org/WUDC/app/LWSP/report.php?pwsid=03-32-010&year=2022>
 
Indicate this website as the as the URL to be scraped. (In other words, read the contents into an `rvest` webpage object.)

```{r set.the.scraping.website}
#2 

#used read_html() and copied/pasted the url
durham.lwsp <- read_html("https://www.ncwater.org/WUDC/app/LWSP/report.php?pwsid=03-32-010&year=2022")

```

3. The data we want to collect are listed below:

* From the "1. System Information" section:
 * Water system name
 * PWSID
 * Ownership
 
* From the "3. Water Supply Sources" section:
 * Maximum Day Use (MGD) - for each month

In the code chunk below scrape these values, assigning them to four separate variables.

>HINT: The first value should be "Durham", the second "03-32-010", the third "Municipality", and the last should be a vector of 12 numeric values (represented as strings)".

```{r scrape.the.data}
#3 

#scrape water system name
Durham <- durham.lwsp %>% 
  html_nodes("div+ table tr:nth-child(1) td:nth-child(2)") %>% 
  html_text()

#scrape PWSID
durham.pwsid <- durham.lwsp %>% 
  html_nodes("td tr:nth-child(1) td:nth-child(5)") %>%
  html_text()

#scrape ownership
durham.ownership <- durham.lwsp %>%
  html_nodes("div+ table tr:nth-child(2) td:nth-child(4)") %>%
  html_text()

#scrape maximum day use per month
durham.mgd <- durham.lwsp %>%
  html_nodes("th~ td+ td") %>%
  html_text()

```


4. Convert your scraped data into a dataframe. This dataframe should have a column for each of the 4 variables scraped and a row for the month corresponding to the withdrawal data. Also add a Date column that includes your month and year in data format. (Feel free to add a Year column too, if you wish.)

>TIP: Use `rep()` to repeat a value when creating a dataframe.

>NOTE: It's likely you won't be able to scrape the monthly widthrawal data in chronological order. You can overcome this by creating a month column manually assigning values in the order the data are scraped: "Jan", "May", "Sept", "Feb", etc... Or, you could scrape month values from the web page...

5. Create a line plot of the maximum daily withdrawals across the months for 2022

```{r create.a.dataframe.from.scraped.data}
#4 

#used data.frame(), set "Month" as a string with the correct order of months,
#set maximum withdrawals as a numeric data type
durham.withdrawals <- data.frame("Month" = c("Jan", "May", "Sep", "Feb", "Jun",
                                             "Oct", "Mar", "Jul", "Nov",
                                             "Apr", "Aug", "Dec"),
                                 "Year" = rep(2022, 12),
                                 "Max_Withdrawals" = as.numeric(durham.mgd))

#mutated columns in data frame to set static attributes as variables, created
#Date column
durham.withdrawals <- durham.withdrawals %>% 
  mutate(City = !!Durham,
         PWSID = !!durham.pwsid,
         Ownership = !!durham.ownership,
         Date = my(paste(Month,"-",Year)))

#5 

#used ggplot() and geom_line() to create line plot of scraped data
durham.2022.plot <- durham.withdrawals %>% 
  ggplot(aes(x=Date, y=Max_Withdrawals)) + 
  geom_line() + 
  labs(title=paste("Max Monthly Day Use for",Durham,"in 2022"),
       x = "Month",
       y = "Withdrawal (MGD)")
durham.2022.plot

```

6. Note that the PWSID and the year appear in the web address for the page we scraped. Construct a function using your code above that can scrape data for any PWSID and year for which the NC DEQ has data. **Be sure to modify the code to reflect the year and site (pwsid) scraped**.

```{r construct.a.scraping.function}
#6. 

#used function with two variables: pwsid and year
scrape.page <- function(url_pwsid, url_year) {
  #scraping web address construction with paste, and retrieval with read_html
  scraped.url <- read_html(paste0("https://www.ncwater.org/WUDC/app/LWSP/report.php?pwsid=",
                                  url_pwsid, "&year=", url_year))
  
  #set element address tags as objects
  scraped.city.tag <- "div+ table tr:nth-child(1) td:nth-child(2)"
  scraped.pwsid.tag <- "td tr:nth-child(1) td:nth-child(5)"
  scraped.ownership.tag <- "div+ table tr:nth-child(2) td:nth-child(4)"
  scraped.mgd.tag <- "th~ td+ td"

  #scrape data items
  scraped.city <- scraped.url %>% html_nodes(scraped.city.tag) %>% html_text()
  scraped.pwsid <- scraped.url %>% html_nodes(scraped.pwsid.tag) %>% html_text()
  scraped.ownership <- scraped.url %>% html_nodes(scraped.ownership.tag) %>%
    html_text()
  scraped.mgd <- scraped.url %>% html_nodes(scraped.mgd.tag) %>% html_text()
  
  #convert to dataframe
  scraped.withdrawals <- data.frame("Month" = c("Jan", "May", "Sep", "Feb", "Jun",
                                             "Oct", "Mar", "Jul", "Nov",
                                             "Apr", "Aug", "Dec"),
                                 "Year" = rep(url_year, 12),
                                 "Max_Withdrawals" = as.numeric(scraped.mgd))
  
  scraped.withdrawals <- scraped.withdrawals %>%
  mutate(City = !!scraped.city,
         PWSID = !!scraped.pwsid,
         Ownership = !!scraped.ownership,
         Date = my(paste(Month,"-",Year)))

  Sys.sleep(2)
  
  return(scraped.withdrawals)
}
```

7. Use the function above to extract and plot max daily withdrawals for Durham (PWSID='03-32-010') for each month in 2015

```{r fetch.and.plot.Durham.2015.data}
#7 

#used scrape.page() function to extract Durham values
durham.2015.withdrawals <- scrape.page("03-32-010", 2015)

#used ggplot() and geom_line() to create line plot
durham.2015.plot <- durham.2015.withdrawals %>%
  ggplot(aes(x=Date, y=Max_Withdrawals)) +
  geom_line() +
  labs(title=paste("Max Monthly Day Use for",Durham,"in 2015"),
       x = "Month",
       y = "Withdrawal (MGD)")
durham.2015.plot

```

8. Use the function above to extract data for Asheville (PWSID = 01-11-010) in 2015. Combine this data with the Durham data collected above and create a plot that compares Asheville's to Durham's water withdrawals.

```{r fetch.and.plot.Asheville.2015.data}
#8 

#used scrape.page() function to extract Asheville values
asheville.2015.withdrawals <- scrape.page("01-11-010", 2015)
asheville.2015.withdrawals

#combine both Durham and Asheville data with bind_rows
withdrawals.2015 <- bind_rows(durham.2015.withdrawals, asheville.2015.withdrawals)

#used ggplot() and geom_line() to create line plot of combined datasets, 
#differentiated between cities by setting color=City under geom_line aesthetics
withdrawals.2015.plot <- withdrawals.2015 %>%
  ggplot(aes(x=Date, y=Max_Withdrawals)) +
  geom_line(aes(color=City)) +
  labs(title=paste("Max Monthly Day Use for Durham and Asheville in 2015"),
       x = "Month",
       y = "Withdrawal (MGD)")
withdrawals.2015.plot

```


9. Use the code & function you created above to plot Asheville's max daily withdrawal by months for the years 2010 thru 2021.Add a smoothed line to the plot (method = 'loess').

> TIP: See Section 3.2 in the "10_Data_Scraping.Rmd" where we apply "map2()" to iteratively run a function over two inputs. Pipe the output of the map2() function to `bindrows()` to combine the dataframes into a single one. 

```{r fetch.and.plot.Asheville.multiyear.data }
#9 

#used rep() to sequence the years from 2010 to 2021, and rep.int() to ensure the
#pwsid numbers are repeated the correct number of times
asheville_years <- rep(2010:2021)
asheville_id <- rep.int("01-11-010", length(asheville_years))

#use map2 with the assigned objects in the correct function (scrape.page) order
asheville.2010.2021.dfs <- map2(asheville_id, asheville_years, scrape.page)

#bind all extracted datasets into one with bind_rows()
asheville.2010.2021.df <- bind_rows(asheville.2010.2021.dfs)
asheville.2010.2021.df

#plot all Asheville withdrawals and apply smoothed line with geom_smooth
asheville.withdrawals.plot <- asheville.2010.2021.df %>%
  ggplot(aes(x=Date, y=Max_Withdrawals, color=Year)) +
  geom_line() +
  geom_smooth(method="loess", se=FALSE) +
  labs(title=paste("Max Monthly Day Use for Asheville from 2010-2021"),
       x = "Year",
       y = "Withdrawal (MGD)")
asheville.withdrawals.plot

```

Question: Just by looking at the plot (i.e. not running statistics), does Asheville have a trend in water usage over time?
> Answer: Asheville has an upward trend in water usage after 2015. The upward trend may have been skewed by several large peaks around 2019, so running statistical analysis would still be helpful. 
>
